\field{統計力学}
\title{エントロピーの別定義}
\comment{今回の話が情報理論と結びつくわけですよ。}
\prev{isobaric}
\next{info}
\create{2008/12/9}
\modify{}


\section{微視的確率とエントロピー}

　前回、定圧集団というものを考えてみたり、
他に有用な手法はないのだろうかと探ってみたりしたことで、
ようやく全体像が見えてきた気がする。
　独立変数の組み合わせを変えて考えていただけであって、
どの方法を使うにしても似たようなパターンが当てはまるのである。

<P>

　残念ながら応用に使えそうな組み合わせはその一部だけだったが、
それは人間側の勝手な都合というやつであって、
途中までの議論に矛盾や間違いがあって放棄されたというわけではないだろう。
　典型的なのは$ (E,V,N) $の変数を全て置き換えて$ (T,p,\mu) $としたパターンである。
　定温、定圧、等化学ポテンシャルの条件下での平衡系で、
容器の中に、ある微視的状態$ i $が実現している確率は、

<tex>
P_i\ =\ \frac{1}{A} \exp\left( -\frac{E_i + pV_i - \mu N_i}{T} \right) \tag{1}
</tex>

と表せて、$ A $は規格化定数だとすると、その時の容器系のエントロピーは、

<tex>
S\ =\ k\,\log_e A\ +\ \frac{U}{T}\ +\ \frac{pV}{T}\ -\ \frac{\mu N}{T} \tag{2}
</tex>

と表せる、というところまでは問題ない。
　さて、他のどの組み合わせを使うにしても、この 2 つの式から
必要のない項を省けばいいだけのことである。

<P>

　そうなると独立変数をどう選ぼうとも、
(1) 式と (2) 式の間にはいつでも必ず似たような形の部分があることになり、
何らかの関連があるようだと気付くだろう。
　そしてそれを数式でちゃんと表してみたいと思うようになるはずだ。
　それをやってみよう。

<P>

　ところで (2) 式の$ U $、$ V $、$ N $というのは平均値の意味であったから、
正確に書くと次のようになるだろう。

<tex>
S\ &=\ k\,\log_e A\ +\ \frac{1}{T}\sum_i P_i E_i \ +\ \frac{p}{T} \sum_i P_i V_i\ -\ \frac{\mu}{T} \sum_i P_i N_i \\
&=\ k\,\log_e A\ +\ \sum_i P_i \left( \frac{E_i}{T} \ +\ \frac{p V_i}{T} \ -\ \frac{\mu N_i}{T} \right) \\
&=\ k\,\log_e A\ -\ k \sum_i P_i \left( -\frac{E_i + p V_i - \mu N_i}{kT} \right)
</tex>

　こうして、(1) 式の形にますます近付いた。
　この共通部分を利用して、(1) 式を無理やりにでもここにねじ込んでやりたい。
　それはそんなに難しいことではないだろう。
　次のようになる。

<tex>
S\ &=\ k\,\log_e A\ -\ k \sum_i P_i \log_e (A P_i ) \\
&=\ k\,\log_e A\ -\ k \sum_i P_i ( \log_e A \ +\ \log_e P_i ) \\
&=\ k\,\log_e A\ -\ k \sum_i P_i\,\log_e A \ -\ k \sum_i P_i \log_e P_i \\
&=\ k\,\log_e A\ -\ k \log_e A\, \sum_i P_i \ -\ k \sum_i P_i \log_e P_i \\
&=\ k\,\log_e A\ -\ k \log_e A \ -\ k \sum_i P_i \log_e P_i \\
&=\ -\ k \sum_i P_i \log_e P_i 
</tex>

　こうして目的は果たされた。
　しかも、何とシンプルな式で表せたことか！
　重要な式なので式番号を付けてもう一度書き直しておこう。

<tex>
S\ =\ -\ k \sum_i ( P_i \log_e P_i ) \tag{3}
</tex>


% ===========================================================

\section{小正準集団にも適用できる}

　ところで、小正準集団の方法の時のように
独立変数を$ (E,V,N) $のままにした場合には
今の話が適用できるだろうか？

<P>

　全状態数が$ W $であり、
全ての状態が等確率で出現すると考えたのだから、
その確率は次のように表せる。

<tex>
P_i = \frac{1}{W}
</tex>

　これを今導いたばかりの関係式 (3) に代入してみると、

<tex>
S &=\ -\ k \sum_{i=1}^{W} \left( \frac{1}{W}\, \log_e \frac{1}{W} \right) \\
  &=\ \ -k\, W \times \frac{1}{W}\, \log_e \frac{1}{W} \\
  &=\ \ k\,\log_e W
</tex>

となり、なんと、最初に紹介したボルツマンの関係式と同じものが出てくる！
　いや、まぁ、そんなに驚くにはあたらないか・・・。
　(2) 式から 2 番目以降の項を外せばそのような形になっているわけだし、
(1) 式の指数の肩を全て取っ払って 0 にすれば、$ P_i = 1/A $となっている。

<P>

　とにかく、(3) 式は以前より広い範囲に適用できる、
エントロピーの定義式だというわけだ。


% ===========================================================

\section{卵が先か、鶏が先か}

　教科書によっては、この (3) 式をいきなり
「エントロピーの定義」として採用し、
これを根拠にしてその後の理論を展開しているものがある。
　$ S = k\,\log_e W $の関係さえもそこから導き出せることを示して、
(3) 式の方が「より根源的なもの」だと説明していたりする。

<P>

　しかしなぜ (3) 式のようなものがいきなり定義として採用されるのか、
その意味が良く分からない。
　その意味を何とか自力で解釈してやろうと頭をひねったが、私には無理だった。
　実は (3) 式もボルツマンによって導入されたらしいのだが、
彼はどういう思考過程でこの式を世に出したのだろう。

<P>

　こういう教科書の説明の仕方は本末転倒ではないかと私は思うのだ。
　おそらくボルツマンはこの式をいきなり思い付いたわけではあるまい。
　新しい計算手法を編み出そうとして、
ここまでやって来たような試行錯誤の中で出来上がった式だろう。

<P>

　とは言うものの、(3) 式にはちゃんと説明可能な意味を見出せるのだ。
　それは情報理論による考察によって導かれるのだが、それは次回にやることにしよう。
　情報理論というのは 20 世紀の半ばに誕生した学問であるが、
理論体系が統計力学と似ていることに気付いて、その結果を取り入れたりもしている。
　とにかく歴史的順序では統計力学よりずっと後のことなのである。


% ===========================================================

\section{誤差はどこへ行ったのか}

　少し気になっていたことの意味が分かり始めたのでここで書いておこう。

<P>

　前回までのやり方で (2) 式のような形を導いた時、
この式の後ろには少しばかりの「無視できる項」がついていて、
厳密な意味で等式が成り立つわけではないのだった。

<P>

　ところが今回導いたような (3) 式をエントロピーの定義として採用すると、
そのような項の存在が初めから出て来ないで、
厳密に成り立っている式のような気がしてしまう。
　この差は一体どこにあるのだろう。

<P>

　実はこの問題は$ S = k\,\log W $という定義を採用した時にもすでに存在していたのである。
　孤立系での状態数が全部で$ W $だけあるというが、
その数の中には容器の片隅に粒子が寄ってしまっている状態や、
全体にムラがあるような状態も全てカウントされている。
　それらはとても平衡状態とは呼べないものである。
　そのような状態の数を$ W $に含めて計算したものをエントロピーと呼ぶべきか、
慎重に吟味して除外したものをエントロピーと呼ぶべきか、どちらがふさわしいだろう。

<P>

　ボルツマンは初め、除外したものをエントロピーだと考えたようである。
　後にギブスが、それらを含めようと除外しようと数としては大差ないというので、
全ての状態数を$ W $として計上するやり方を採用して普及させたらしい。

<P>

　このように、エントロピーの値が数学的に厳密な等式で定義されるものかどうかという心配は、
それ以前の解釈からしてもすでにずれがあるのであり、
もし考え方の違いがあったとしても誤差の中に埋もれるのである。


