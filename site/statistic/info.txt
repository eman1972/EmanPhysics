\field{統計力学}
\title{情報エントロピー}
\comment{情報科学と物理学はいつか融合するような気がする。}
\prev{another_def}
\next{equipartition}
\create{2009/1/20}
\modify{}


\section{二種のエントロピー}

　情報科学の分野にもエントロピーという用語が出てくる。
　これは情報量の大きさ（情報の確かさ）を表すために導入された概念である。
　そもそもは統計力学とは無関係のアイデアだったのだが、
統計力学に出てくるエントロピーの概念に似ていることに気付いて
同じ名前を採用することになった。
　物理学のエントロピーと区別するために「\red{情報エントロピー}」と呼ばれることがある。

<P>

　なぜそのような異分野の概念をここで説明しようとしているかというと、
最近、この「情報」というものが物理学と深い関わりを持とうとしてきているような
気がするからである。
　ブラックホールについてホーキングが新しい理論を打ち立て、
それに関係して、ブラックホールの表面積がエントロピーを表しているだの、
ブラックホールに吸い込まれた物質の情報は永久に失われるのかどうかだのといった問題が
語られるようになってきた。
　どうやら最先端の研究では、
熱力学的なエントロピーと情報のエントロピーとが同列に語られているようなのである。

<P>

　実はそれ以前から、情報エントロピーと熱力学的エントロピーについて、
「<b>それらは区別する必要のない全く同じものだ</b>」と考える意見と、
「<b>形式が同じというだけの全く別概念だ</b>」と考える意見とが存在している。
　私は前者寄りの立場であり、
なぜそう考えられるのかを説明したいと考えている。

<P>

　両者を関連付ける前に、この記事では情報エントロピーの説明を試みることにしよう。

% ===========================================================

\section{コンピュータの動作}

　コンピュータはスイッチの ON と OFF の二つの状態の組み合わせで情報を処理する。
　複数のスイッチの状態の組み合わせによって、
それにつながる別のスイッチをどちらの状態に切り替えるかを決めたりしている。
　コンピュータというのは複雑に接続されたスイッチの塊だというわけだ。

<P>

　電磁石を使ってスイッチを切り替える装置をリレーと呼ぶのだが、
昔はそのリレーを組み合わせて計算していた。
　昔のアニメなんかではコンピュータが「ガシャガシャガシャガシャ・・・」と
音を立てて計算していたりしたが、このリレーの騒音を表現したものだ。
　実は私はその本物を見たことがない。
　イメージ通りの音を出す動画がないかを探したが、<a href="https://jp.youtube.com/watch?v=KQ9spCW8ddk">ここ</a>や<a href="https://jp.youtube.com/watch?v=67jLony0mXg">ここ</a>なんかが面白い。

<P>

　やがてそのスイッチ群は真空管で置き換えられ、
次にトランジスタ、そして今ではそれらを高密度に集めた半導体チップが
その役割を果たしている。
　要するに同じ働きをするものならば何を使ってもいいというわけだ。
　趣味でなら<a href="http://www.blikstein.com/paulo/projects/project_water.html">水コンピュータ</a>なんてものもある。
　（なぜこんなことを書くかというと、シリコンという元素自体に
知性の源が存在しているかのように信じてしまっている人もいるようだからだ。）

<P>

　そのスイッチの ON-OFF 状態を 1 と 0 を使って表すと 2 進法の考え方が使えて便利である。
　例えば、0 ～ 7 の 8 つの数値を表したければ、(000) (001) (010) (011) (100) (101) (110) (111) といった具合に、3 つのスイッチの組み合わせで表現できることになる。

<P>

　現在の主流技術に限れば、情報を表す基本単位はこのスイッチである。
　この基本単位を「\red{ビット}」と呼ぶ。
　これは binary digit の略であるが、「少量」の意味を表す bit と掛けた洒落でもある。
　今見たように、3 ビットあれば、8 通りのデータが区別できるし、
$ n $ビットあれば$ 2^n $通りのデータが表せるというわけだ。

% ===========================================================

\section{ビットと状態数}

　このビットは YES か NO を表していると考えてもいい。
　誰かが頭の中で 0 ～ 7 の数字のうちのどれかを思い浮かべていて、
自分はその数字を当てようとしているとする。
　そして YES か NO かで答えられる質問を相手に尋ねることが許されているとしよう。
　事前に何の情報も与えられていなければ、
最低何回の質問でその数値を完全に特定できるだろうか。
　答えは 3 である。

<P>

　8 通りの曖昧さの中から予備知識なく完全な知識を得ようとする時、
3 回分の YES-NO 質問の答えに相当する情報が必要になるのである。

<P>

　もし 6 ビットあれば、$ 2^6 $= 64 通りの状態を区別できるだろう。

<P>

　3 ビットと 6 ビットを合わせれば 9 ビットだが、
これによって幾つの状態が表せるようになるかというと、$ 2^3 \times 2^6 $= 512 通りである。
　ビット数を足し算する時、その合計のビットによって表すことができる状態数は、
それぞれのビット数で表すことのできる状態数の掛け算によって求めることができる。

<P>

　これはボルツマンの関係式に非常に良く似た状況である。
　ビット数を$ S $、それによって表すことのできる状態数を$ W $とするならば、

<tex>
S \ =\ k\,\log_e W
</tex>

という関係があるということだ。
　この場合の定数$ k $は幾つになるだろう？
　いや、わざわざこんな式を持ち出さずとも、
次のように対数の底を 2 にしておけば$ k $を使わなくて済む。

<tex>
S \ =\ \log_2 W
</tex>

　しかしどうしても統計力学のエントロピーと同じ形にして
ボルツマン定数との比較をしたいというのなら次のように変形をしてやればいい。

<tex>
S \ =\ \log_2 W\ =\ \frac{\log_e W}{\log_e 2}\ \kinji\ 1.44 \log_e W
</tex>

　「底の変換公式」があって良かったと実感したのは高校で習って以来これが初めてだ。
　これでこの場合の$ k $の値が約 1.44 であることが分かった。
　ボルツマン定数が$ 1.38 \times 10^{-23} $であったのとは大違いだ。

<P>

　もうかなり疑われているかも知れないので早めに白状しておくと、
今話している$ S $というのが情報エントロピーなのである。
　しかしこれだけの説明だと「なーんだ、情報エントロピーというのは、
つまりビット数のことだったのか」と
小さな理解をされて終わってしまう惧れがある。
　それはそれで間違ってはいないのだが、
このことから大きな勘違いをしてしまっている人が実に多い。
　その意味についてもう少し詳しい説明をしておく必要があるだろう。

% ===========================================================

\section{エントロピーの意味}

　よく見かける大きな勘違いとはこういうものである。
　ビット数$ S $が多いほど表現できる状態の数$ W $も多い。
　それで、情報エントロピーが大きいほど情報量が多いと思い込んでしまっているのである。
　情報理論では情報量はそのようには定義されていない。
　むしろ逆に考えた方がいいくらいだ。

<P>

　$ W $が大きいほど、それは正しい答えにたどり着くための選択肢の幅が広くて、
知識の曖昧さが大きいことを表している。
　つまり、$ S $が大きいのは情報が足りない状態を示しており、
情報を得ることによって$ S $の値を下げる必要がある。
　$ W $が 1 になったとき、つまりそれは$ S $が 0 になったときということだが、
それは選択すべき答が唯一つに確定した状態であり、情報が十分にあると言える。

<P>

　情報エントロピーが$ S $であるような状態にどれだけの情報量$ I $を注入すれば
情報エントロピーを 0 に持っていけるだろうか、という考え方をするならば、
それは$ S - I = 0 $という引き算で計算ができて、$ I = S $だと言えるのである。
　$ I $はその時必要な情報量をビット数で表したものだと考えればいいだろう。

<P>

　まぁ、いつもかも$ S $を 0 に持って行くような問題ばかりではない。
　状態$ S $のところへ情報$ I $を注入することで曖昧さはどの程度にまで減るか、
ということも論じるのである。
　情報エントロピーの減少分が、情報量を意味しているというわけだ。

<P>

　ところで、$ W $は必ず 2 のべき乗の数値だとは限らないから、
$ S $だって常に整数だというわけではない。
　それを「ビット」という単位で呼ぶのは少しだけ抵抗があるので、
正式には「\red{シャノン}」という単位名を使うことが国際規格で決まっているらしい。
　しかしこの呼び名はまだそれほど普及していないようだ。

% ===========================================================

\section{ちょっと面倒になってきた}

　簡単に済むと思っていたことの説明が意外に長くなってしまってなかなか進まないので、
果たして私がここで情報理論について詳しく話すことに意味があるのだろうかという
疑いが生じ始めた。
　しかしやりかけたことはやってしまおう。
　もし私が情報理論について本格的な説明をするならもっと別のアプローチを取っただろうが、
ここでは物理のエントロピーと比較するためにやっているので、
必要最小限の基本的な内容にとどめることにする。
　それでもまだ長くなりそうだ。

% ===========================================================

\section{確率と情報}

　さてこれから、知識の曖昧さを、確率の話と結びつけよう。
　なぜそうするのか。
　そもそも「情報」や「知識」と言っても色んな意味があるのであり、
ある人にとって重要な情報が別の人にとってはゴミ屑に過ぎないということが良くある。
　そういう、人の価値観やデータの解釈によって左右されるような意味の情報ではなく、
客観的な基準で測れる情報の話に議論を限定しようとしているのである。

<P>

　サイコロを例に取るのが分かりやすい。
　サイコロを振る前には次に 1 ～ 6 のどの目が出るかは分からないので、
$ W = 6 $に相当する曖昧さがあるわけだ。
　サイコロを振った後ではこのことについての知識は確実になるので、
このサイコロを振ることで得られる情報量は、$ \log_2 6 $であると言える。

<P>

　サイコロのそれぞれの目の出る確率は 1/6 であるが、
もっと目の数の多い 8 面体サイコロや 20 面体サイコロに拡張してもいい。
　どの目の出る確率も等しい時、確率$ P $と$ W $との間には$ P = 1/W $という関係があるだろう。　このことを元にして確率と情報の関係の話に持っていくことができる。

<P>

　例えば、複数の事象の内のどれか一つだけが実現するという状況があり、
そのどれもが等しい確率$ P $で起こることになっているとする。
　その時、実際にどれが実現したかを知った時に得られる情報$ I $は、

<tex>
I\ =\ \log_2 \frac{1}{P}\ =\ -\log_2 P
</tex>

と表せると言えるだろう。
　この結果を知る前の情報エントロピー$ S $は、これと同じ、$-\log_2 P $だったと
いう事でもある。


% ===========================================================

\section{確率に差がある場合}

　では少し設定を変えてみよう。
　複数の事象の内のどれか一つだけが実現するという状況は先ほどと同じである。
　しかしそれぞれの事象が実現する確率には違いがあるとする。
　これで話はどう変わるだろうか。

<P>

　仮に、その中の一つだけが他と比べてダントツに実現確率が高かったとして、
実際にそれが起きたとき、我々はその結果をどう思うだろう？
　多分、まったく驚きはしないだろうと思うのだ。
　なぜなら、それが起きるだろうことは予め大方の予想が付いていたことになるからだ。
　つまりそのことについてのある程度の情報を初めから持っていたわけであり、
新たに追加された情報は少ないと言える。
　極端な話、ある事象が起きる確率が 100 \% で、他が全て 0 \% だとしたら、
結果を知る前にすでに結果が分かっているのだから、新たに得る情報は 0 だと言える。

<P>

　このように、一つの事象が実現したのを知ったときに得る情報は、
その事象に対して予め設定されていた確率によって違ってくる。
　そういう意味を織り込みたいのだが、
しかし一体、どういう式で表したらいいだろうか？

<P>

　それを説明するのは少し面倒なのだが、結論を先に言ってしまうと、

<tex>
I(P_i)\ =\ \log_2 \frac{1}{P_i}\ =\ -\log_2 P_i
</tex>

と定義しておくのが最も都合が良い。
　$ P_i = 1 $を代入すれば今の話を満たしているし、
先ほど考えた式の素直な拡張にもなっている。

<P>

　それだけではない。
　独立した二つの事象が起こる確率をそれぞれ$ P_i $、$ P_j $とした時、
それらが続けざまに起こる確率は$ P_i P_j $という積で表されるが、
それによって得られる情報量が、それぞれで得た情報の和で表せるということも言える。

<tex>
I(P_i P_j )\ =\ I(P_i) + I(P_j) 
</tex>

　この関係を満たせるのは$ P_i $が連続である時にはこの定義だけだと言えるので、
この定義にはかなり説得力があるだろう。

<P>

　しかしこれはとても奇妙ではないだろうか。
　確率的に起こることの結果次第でそこから得られる情報量が違うというのであれば、
それを知る前の情報エントロピーは一体幾つだったと言えるのだろうか。
　結果を知ったことで知識は確実となり、
情報エントロピーは 0 になるという説明を先ほどはしたのだった。

<P>

　情報エントロピーの解釈を少し修正した方がいいかも知れない。


% ===========================================================

\section{平均情報エントロピー}

　今考えたような確率に差のある状況の場合、
結果を知る前のエントロピーについては確かなことは言えないのである。
　なぜなら、結果としてどれが実現して、
そこからどれだけの情報が得られるかは確率任せになっているからだ。
　しかし、次のようなことは言えるだろう。
　このようなことを何度も何度も繰り返すとき、
そこから平均して得られる情報量は幾らになるか、というものだ。

<P>

　それぞれの事象が実現する確率が$ P_i $で、
それが実現した時には$ -\log_2 P_i $の情報が得られるのだから、
その平均は次のような計算をすることで求められるだろう。

<tex>
I\ =\ - \sum_i P_i\,\log_2 P_i
</tex>

　結果を知る前のエントロピーも、これを根拠に同じ式で決めることにしよう。

<tex>
S\ =\ - \sum_i P_i\,\log_2 P_i
</tex>

　それでこれを「\red{平均情報エントロピー}」と呼んだりする。
　この式は、前回出てきた統計力学でのエントロピーの定義と同じ形のものである。
　この式にたどり着きたいが為に、ここまで情報科学の講義みたいなことをしてきたのだった。

<P>

　ここではかなり定性的な説明をしたが、
この式を求める方法は他にも幾つかあって、
それを使った方が納得しやすいかも知れない。
　ちょっとやってみよう。

% ===========================================================

\section{平均エントロピーの他の説明}

　$ k $種類のアルファベットを使って長さ$ n $文字の文を作るとする。
　各アルファベットの出現頻度は
$ ( P\sub{1}, P\sub{2},\cdots, P\sub{k}) $と決まっているとする。
　つまり$ n $が十分に大きければ、その文中に含まれる各アルファベットの文字数は、
$ ( nP\sub{1}, nP\sub{2},\cdots, nP\sub{k}) $となるわけだ。

<P>

　では各アルファベットをその数だけ使って
作ることのできる文の総数$ M $は幾つになるだろうか。
　まぁ、こんな感じだ。

<tex>
M\ =\ \frac{n!}{(nP\sub{1})!\cdot(nP\sub{2})!\cdots(nP\sub{k})!} 
</tex>

　これだけの数の文が、どれも等確率で出現すると言えるのだから、
その文の一つを得た時の情報量は、文句なく、始めの方の考え方で計算できるだろう。
　変形の途中でスターリングの公式を使うことにする。

<tex>
S\ &=\ \log_2 M \\
&=\ \log_2 n!\ -\ \sum_{i=1}^k \log_2(n P_i)! \\
&=\ n( \log_2 n - 1) \ -\ \sum_{i=1}^k nP_i \{ \log_2(n P_i) - 1 \} \\
&=\ n \log_2 n - n \ -\ \sum_{i=1}^k nP_i \log_2 n P_i + \sum_{i=1}^k nP_i \\
&=\ n \log_2 n - \cancel{n} \ -\ n \sum_{i=1}^k P_i \log_2 n P_i + \cancel{n \sum_{i=1}^k P_i} \\
&=\ n \log_2 n \ -\ n \sum_{i=1}^k P_i (\log_2 n + \log_2 P_i ) \\
&=\ n \log_2 n \ -\ n \sum_{i=1}^k P_i \log_2 n \ -\ n \sum_{i=1}^k P_i \log_2 P_i  \\
&=\ \cancel{n \log_2 n} \ -\ \cancel{n \log_2 n \sum_{i=1}^k P_i} \ -\ n \sum_{i=1}^k P_i \log_2 P_i  \\
&=\ -\ n \sum_{i=1}^k P_i \log_2 P_i 
</tex>

　$ n $文字あるからこのような式になるのであって、
一文字を得るごとの平均情報量は、先ほどの平均エントロピーと同じだというわけだ。


% ===========================================================

\section{エントロピーの意味}

　さあ、これくらいでいいだろう。
　ここまでの話を参考にして、統計力学のエントロピーの意味を
再解釈できないかを考えてみよう。

<P>

　どちらのエントロピーも、状態の多さ、曖昧さ、定まらなさを表しているようだ。
　統計力学におけるエントロピーというのは、
マクロな意味での状態を一つに定めた時、
それを実現するためのミクロな状態がどれだけ多様性に富んでいるかを表していると言える。
　つまり、その多数ある状態の「どれでも良さ」「選択の幅の広さ」を表しているわけだ。

<P>

　では各状態の出現確率が異なるとした時のことも考えてみよう。
　正準集団や大正準集団などを考えた時がそうだったが、
その時のエントロピーは今回出てきた平均エントロピーと同じ形になったのだった。
　ここから何が読み取れるだろう。

<P>

　ある微視的状態に注目すると、エントロピーが大きいほど、その状態は選ばれにくい。
　「選ばれにくい」というのは、その状態が何ら特別ではなくて、
代わりはいくらでもいる・・・他のどれでもいいということだ。
　各微視的状態の出現確率は違っているので、「どれでも良さ」には差がある。
　エントロピーは、その「どれでも良さ」の平均値を表しているというわけだ。

<P>

　あまり面白い結論は導けなかったが、まぁ、こんなものか。


% ===========================================================

\section{次回予告}

　ここまでに考えた限りにおいては、情報エントロピーと
熱力学的なエントロピーとを完全に同一視するほどの積極的な根拠はまるでないと言える。
　ただ理論の構造が似ているために、一部を重ね合わせて議論することができるだけだと
いう見方ができるだろう。

<P>

　しかし情報や知識を駆使することによって、
熱力学的なエントロピーを下げることができるような具体的事例が考えられるとしたらどうだろう。
　次回はそういう話を考えていこう。
　それはエントロピー増大則に反することになりはしないだろうか。

<P>

　情報理論が登場したもともとの動機は、
通信を行うときの途中の雑音に対してデータがどれほど耐えうるかを調べることであった。
　曖昧さを含んでしまったデータからどれほどの情報が取り出せるかという研究の成果である。
　情報エントロピーも、放っておけば、
雑音という付加逆過程が加わることにより、増大する一方なのだ。
　ここにもエントロピー増大則という共通点が存在する。

<P>

　果たして、二種のエントロピーは同一のものだと言えるのだろうか、
それとも似ているだけで無関係なのだろうか。

