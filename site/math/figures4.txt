\field{物理数学}
\title{不偏分散の疑問}
\comment{式を使う以外の方法でも考えてみたが、<br>本質ではなかったから書くのはやめた。}
\prev{figures3}
\next{semilog}
\create{2016/12/27 16:00}
\modify{}



\section{期待値の性質}

　前回の記事の数学的な補足である。
　不偏分散を計算するときに$ n-1 $で割るのはなぜなのかという話である。

\note{　大抵の統計の入門書では軽く誤魔化しているので「なぜなのか」とすごく気になってしまう人も多いのではないかと思っていたが、
前回の話を読んでしまうと、「ああ、数学的に多分そうなんだろうな」と思えてきて、
もうあまり気にならなくなっていたりは・・・しないかな？}

　本題に入る前に少しだけ準備をしておきたい。
　前回は期待値というものを母集団の平均値という意味だけで紹介したのだった。
　今回はこの計算を数学的な道具として積極的に使ってみたいので次のように表現してみる。

<tex>
E[x] \ \equiv\ \int_{-\infty}^{\infty} x \, f(x) \diff x
</tex>

　これは変数$ x $が出現率$ f(x) $に従って確率的に得られるときの期待値を意味している。
　もし$ E[y] $というのが出てきて、$ y $の値の出現率の分布が$ g(y) $である場合には次のような意味になるものだとする。

<tex>
E[y] \ \equiv\ \int_{-\infty}^{\infty} y \, g(y) \diff y
</tex>

　もし$ E[x+y] $というものが出てきたとすると、
これは出現率$ f(x) $であるところからたまたま引いた$ x $という値と、
出現率$ g(y) $であるところからたまたま引いた$ y $という値の和を作る作業を何度も何度も行い、
それら全ての平均値を意味するようにしたい。
　これを計算するには、$ x = x\sub{0} $かつ$ y= y\sub{0} $という値を拾う確率密度が
$ f(x\sub{0})\,g(y\sub{0}) $であることを使ってやればいい。

<tex>
E[x+y] \ &=\ \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (x+y) \, f(x) \, g(y) \diff x \diff y \\
&=\ \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \Big[ x \, f(x) \, g(y) + y \, f(x) \, g(y) \Big] \diff x \diff y \\
&=\ \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \, f(x) \, g(y) \diff x \diff y \ +\ \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y \, f(x) \, g(y) \diff x \diff y \\
&=\ \int_{-\infty}^{\infty} x \, f(x) \diff x \, \int_{-\infty}^{\infty} g(y) \diff y \ +\ \int_{-\infty}^{\infty} y \, g(y) \diff y \int_{-\infty}^{\infty} f(x) \diff x \\
&=\ \int_{-\infty}^{\infty} x \, f(x) \diff x \ +\ \int_{-\infty}^{\infty} y \, g(y) \diff y \\[5pt]
&=\ E[x] \ +\ E[y]
</tex>

　結果だけ見ると、なんとまぁ単純な関係である。
　変形の途中で、それぞれ単独の確率密度だけで全範囲を積分すると 1 になることを使っている。
　さらに定数$ a $、$ b $などを入れて拡張してやろう。
　同様の計算をしてやることで、
次のような関係が成り立っていることも言えるだろう。

<tex>
E[ax+by] \ =\ a\,E[x] \ +\ b\,E[y]
</tex>

　こんなにきれいな関係が成り立っているのを見ると、
$ E[xy] $というものがどうなるかというのも気になってくる。
　実はこれも同じような考えで簡単に計算ができる。

<tex>
E[xy] \ &=\ \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xy\, f(x)\, g(y) \diff x \diff y \\
&=\ \int_{-\infty}^{\infty} x\, f(x) \diff x \int_{-\infty}^{\infty} y\, g(y) \diff y \\[5pt]
&=\ E[x] \ E[y]
</tex>

　非常に分かりやすい関係だ。
　さらに考えると定数$ a $を使って次のような関係も言える。

<tex>
E[a] \ =\ a
</tex>

　これは毎回確実に$ a $という定数値が出る場合の期待値だということで受け入れよう。


<P>

　さて、母分散$ V $というのは次のような定義であった。

<tex>
V \ \equiv\ \int_{-\infty}^{\infty} (x-\mu)^2 f(x) \diff x
</tex>

　前回と少し違っているのは母分布の期待値を$ E $ではなく$ \mu $で表すことにしたからである。
　今回は$ E[x] $を多用するので区別しやすくしておいた。
　この式は次のように表現することも出来るだろう。

<tex>
V \ =\ E\Big[ (x-\mu)^2 \Big]
</tex>

　ところがこの式の右辺を先ほどの性質を使ってバラそうとするとおかしなことが起こる。

<tex>
E\Big[(x-\mu)^2\Big] \ &=\ E\Big[(x-\mu)\Big] \, E\Big[(x-\mu)\Big] \\
&=\ (E[x] - \mu)(E[x] - \mu) \\
&=\ (\mu-\mu)(\mu-\mu) \\
&=\ 0
</tex>

　この変形はやってはいけないのだ。
　上で説明した性質は$ x $と$ y $とがそれぞれ独立に得られるものだという前提を使っている。
　ところが今回は$ x $の値を得て 2 乗した。
　同じ値どうしを掛け合わせたことになる。
　つまり、独立に得た二つの値を使っているという条件が満たされていないのである。

<P>

　この辺りのことを気をつけながら、今回の本題に入っていくことにしよう。


% =======================================================================

\section{なぜ不偏分散は n-1 で割るのか}

　知りたいのは「なぜ不偏分散は$ n-1 $で割るのか」ということだった。
　不偏分散$ W $は次のように定義されていたのだった。

<tex>
W \ =\ \frac{1}{n-1} \sum_{i=1}^n (x_i - m )^2
</tex>

　$ m $は$ n $回の実験値の平均値である。
　これも混乱を防ぐために、前回とは記号を変えてある。

<P>

　さて、疑問を言い換えれば、「なぜ$ n-1 $で割るとうまくいくのか」ということであり、
さらに言い換えれば、不偏分散$ W $の期待値$ E[W] $が母集団の分散に等しいのはなぜか、ということだ。
　式で表すと次のようになる。

<tex>
E[W] \ =\ V
</tex>

　この式が成り立つことが示されれば、納得するしかない。
　では左辺の変形を進めよう。

<tex>
E[W] \ &=\ E\left[ \frac{1}{n-1} \sum_{i=1}^n (x_i - m )^2 \right] \\
&=\ \frac{1}{n-1} \, E\left[ \sum_{i=1}^n (x_i - m )^2 \right] \\
&=\ \frac{1}{n-1} \, E\left[ \sum_{i=1}^n (x_i - \mu + \mu - m )^2 \right] \\
&=\ \frac{1}{n-1} \, E\left[ \sum_{i=1}^n \Big( (x_i - \mu) - (m - \mu) \Big)^2 \right] \\
&=\ \frac{1}{n-1} \, E\left[ \sum_{i=1}^n \Big( (x_i - \mu)^2 - 2(x_i - \mu)(m - \mu) + (m - \mu)^2 \Big) \right] \\
&=\ \frac{1}{n-1} \, E\left[ \sum_{i=1}^n (x_i - \mu)^2 \ -\ 2(m - \mu)\sum_{i=1}^n (x_i - \mu) \ +\ n(m - \mu)^2 \right] \\
&=\ \frac{1}{n-1} \, E\left[ \sum_{i=1}^n (x_i - \mu)^2 \ -\ 2(m - \mu)\left(\sum_{i=1}^n x_i - n\mu \right) \ +\ n(m - \mu)^2 \right] \\
&=\ \frac{1}{n-1} \, E\left[ \sum_{i=1}^n (x_i - \mu)^2 \ -\ 2(m - \mu)\left(nm - n\mu \right) \ +\ n(m - \mu)^2 \right] \\
&=\ \frac{1}{n-1} \, E\left[ \sum_{i=1}^n (x_i - \mu)^2 \ -\ 2n(m - \mu)^2 \ +\ n(m - \mu)^2 \right] \\
&=\ \frac{1}{n-1} \, E\left[ \sum_{i=1}^n (x_i - \mu)^2 \ -\ n(m - \mu)^2 \right] \\
&=\ \frac{1}{n-1} \, \left( E\left[ \sum_{i=1}^n (x_i - \mu)^2 \right] \ -\ n E\Big[(m - \mu)^2\Big] \right)
</tex>

　ここらで、カッコ内の二つの項を分けて説明しよう。
　まず最初の項は次のような変形が可能だ。

<tex>
E\left[ \sum_{i=1}^n (x_i - \mu)^2 \right] \ &=\ \sum_{i=1}^n E\left[ (x_i - \mu)^2 \right] \\
&=\ \sum_{i=1}^n V \\[5pt]
&=\ n \, V
</tex>

　説明は要らない気がしてきた。
　$ x_i $は測定値を意味しているが、母集団の期待値$ \mu $との差を使って期待値を計算しているのだから、
それは母分散$ V $と同じ値になるのは当然だ。

<P>

　次の項に含まれる$ m $は$ n $回の測定の平均値だという定義に書き戻して変形してやることにする。

<tex>
E\Big[(m - \mu)^2\Big] \ &=\ E\left[\left(\frac{1}{n}\sum_{i=1}^n x_i - \mu \right)^2\right] \\
&=\ E\left[\left(\frac{1}{n}\sum_{i=1}^n x_i - \frac{1}{n}\sum_{i=1}^n \mu \right)^2\right] \\
&=\ E\left[ \frac{1}{n^2} \left( \sum_{i=1}^n (x_i - \mu) \right)^2 \right] \\
&=\ E\left[ \frac{1}{n^2} \left( \sum_{i=1}^n (x_i - \mu) \right)\,\left( \sum_{j=1}^n (x_j - \mu) \right) \right] \\
&=\ \frac{1}{n^2} \, E\left[ \sum_{i=1}^n \sum_{j=1}^n (x_i - \mu)(x_j - \mu) \right] \\
&=\ \frac{1}{n^2} \, \sum_{i=1}^n \sum_{j=1}^n \, E\Big[ (x_i - \mu)(x_j - \mu) \Big] 
</tex>

　とてもややこしい感じがしてきたが、ここが今回の一番の山場で、考えるのが面白いところだ。
　ここで$ i $と$ j $のあらゆる組み合わせの和を考えている。
　この$ i $と$ j $は$ n $回の測定の中で何番目に取得したデータであるかという意味だ。
　先ほど分散の式をバラそうとしておかしなことが起こってしまった場面を思い出してみてほしい。
　$ i \neq j $ならば、$ i $番目に取得した測定値と$ j $番目に取得した測定値は独立しているから、
先ほどのような変形をしてもいいのである。
　そして先ほども見たように、値は 0 になる。
　$ i = j $となる場合だけは残さなくてはならない。
　それで、続きは次のようになる。

<tex>
&=\ \frac{1}{n^2} \, \sum_{i=1}^n \, E\Big[ (x_i - \mu)(x_i - \mu) \Big] \\
&=\ \frac{1}{n^2} \, \sum_{i=1}^n \, E\Big[ (x_i - \mu)^2 \Big] \\
&=\ \frac{1}{n^2} \, \sum_{i=1}^n \, V \\
&=\ \frac{1}{n^2} \, n \, V \\
&=\ \frac{1}{n} \, V 
</tex>

　さあ、これらを元の式に戻して結果を見よう。

<tex>
E[W] \ &=\ \frac{1}{n-1} \, \left( E\left[ \sum_{i=1}^n (x_i - \mu)^2 \right] \ -\ n E\Big[(m - \mu)^2\Big] \right) \\
&=\ \frac{1}{n-1} \, \left( n\,V \ -\ n \frac{1}{n} \, V \right) \\
&=\ \frac{1}{n-1} \, \left( n\,V \ -\ V \right) \\
&=\ \frac{1}{n-1} \, (n-1)\,V \\[5pt]
&=\ V
</tex>

　予告通りである。
　めでたし、めでたし。

% =======================================================================

\section{なぜ不偏分散をnで割ったものの期待値が平均値の分散になるのか}

　次に考える疑問を式で表してみよう。
　不偏分散$ W $を測定回数$ n $で割ったものは前回の記事では$ X $と表していたのだった。
　もし$ n $回の測定を何度も何度も繰り返すと、そのたびに違った値の$ X $を得るだろう。
　その期待値は$ E[X] $と表せる。
　それが平均値$ m $の分散に等しいというのである。

<tex>
E[X] \ =\ V_m
</tex>

　状況を式に表そうとしてとっさに$ V_m $という記号を作ってしまったが、
この意味をはっきりさせないといけない。
　平均値の分散とは何だっただろうか。
　$ n $回セットの測定を何度も何度も繰り返すたびに、
$ n $個のデータを使って平均値を算出したものが$ m $であり、その値は毎回異なっている。
　その平均値$ m $ばかりを集めて作った平均値と、毎回の平均値との差を 2 乗して足し合わせて、
全部の平均値の個数で割った値である。
　平均値の個数は有限ではなく、無限回行うのである。

<P>

　これをどうやって式で表そうか。
　平均値ばかりを集めて作った平均値というのは、母集団の平均値$ \mu $に等しくなるだろう。
　$ E[m] = \mu $である。
　$ m $はガウス分布に従って出現する変数なのだった。
　ガウス分布に従って出現する値$ m $を使って$ (m-\mu)^2 $を計算して、その期待値を算出するのだから、

<tex>
V_m \ =\ E\Big[(m-\mu)^2 \Big]
</tex>

と表せば良いだろう。
　上の方で$ V = E[(x-\mu)^2] $と表したのと同じ原理だ。
　ところがこの右辺は、先ほどの疑問を解決するための式変形の途中で既に出てきている。
　ちょっと探してみてほしい。
　「$ m $を定義に書き戻して変形してやることにする」と言っていた部分だ。
　次のような結論が出ているはずである。

<tex>
E\Big[(m-\mu)^2 \Big] \ =\ \frac{1}{n} V
</tex>

　さらに先ほど解決した疑問は$ E[W] = V $という式で表されていたから、
右辺の$ V $を$ E[W] $で置き換えよう。
　ここまでの変形を一気に書き並べると次のようになる。

<tex>
V_m \ &=\ E\Big[(m-\mu)^2 \Big] \\
&=\ \frac{1}{n} V \ =\ \frac{1}{n} E[W] \\
&=\ E\left[ W/n \right] \\[3pt]
&=\ E[X]
</tex>

　$ V_m $が$ E[X] $になることが示せてしまった。
　疑問はあっけなく解決だ。
　めでたし、めでたし。

% =======================================================================

\section{なぜ X が平均値の分散に近い値だと信じていいのか}

　$ E[X] $が平均値の分散$ V_m $に等しいということは導き出せた。
　しかし$ n $回きりの測定で得られるのは$ E[X] $ではなく、$ X $である。
　この$ X $がなぜ$ V_m $に近い値になっていると信じられるのかというのが最後の疑問だ。

<P>

　$ n $回セットの測定ごとに得られる$ X $の値はどのような分布で出現するのだろうか。
　それが分かれば疑問は解決する。
　ある値の周辺でとても狭く尖った分布になっていれば、
滅多なことでは大きくハズレた値は得られないはずで、信用ができると言えるだろう。

<P>

　その辺りを探るヒントはないものかと探し回ってみたところ、
やっとのことで使えそうなヒントが見付かった。
　「$ \color{red}{\chi^2} $\red{(カイ2乗)分布}」と呼ばれるものだ。
　これは統計学の教科書に必ずと言っていいほど出てくるものだが、
今から調べようとしていることとは異なる目的で紹介されることが多いもので、
その存在に気付くのが大変遅れてしまった。

<P>

　これは標準正規分布に従って独立に$ n $個の変数$ x_i $を得て

<tex>
z \ =\ \sum_{i=1}^n x_i^2
</tex>

という値を計算したときの、この$ z $の分布がどうなっているかを意味するものであるらしい。
　標準正規分布というのは 0 のところにピークがあるような標準偏差が 1 のガウス分布のことである。
　これは大変に都合がいい。
　なぜなら、我々の今の目的からすると、
$ n $回の測定を行ったときに、毎回母集団の期待値$ \mu $を引いてから 2 乗して和を取ることに相当するからで、
ピークからの差の 2 乗和という意味になっている。
　これを$ n-1 $で割れば不偏分散の意味になるし、さらに$ n $で割れば、今知りたい$ X $になるわけだ。

<tex>
X \ =\ \frac{z}{n(n-1)}
</tex>

　ただ少し違うのは、我々はこれまで母集団の分布がガウス分布だとは仮定してこなかったし、
標準偏差も 1 ではなかった。
　しかし、標準偏差の違いはグラフの横幅のスケールが変わるだけの話であるし、
今の目的は$ X $の分布の様子がどうなっているかが大雑把に確認できればいいだけなので、
母集団の分布がガウス分布だと仮定して話を進めることにしよう。

<P>

　このカイ二乗分布の式を求めることはここではやらない。
　式を見てもらえば今はそこまで手を出さない方がいい理由が分かってもらえるだろう。

<tex>
h(z,n) \ =\ \frac{(1/2)^{n/2}}{\Gamma(n/2)} z^{n/2-1} e^{-z/2}
</tex>

　<a href="../statistic/gamma_func.html">ガンマ関数</a>というちょっと特殊な関数まで使われているが、
最初の分数の部分は全体を積分したときに 1 になるようにするための調整部分だからあまり気にすることもない。
　$ z $は負の値にはなりようがないので、$ z<0 $の領域では$ h(z,n) = 0 $である。

\image{./figures4/figures4a.png,カイ二乗分布のグラフ}

　$ n $が増えるほど$ z $が増えるのは当然なので、だんだんと横に広がっている。
　$ h(z,n) $を$ z $で微分して 0 になるところを求めてみれば分かることだが、
ピークの位置は$ n-2 $であり、だんだん右へと向かっている。
　さあ、これを$ n-1 $で割ることで不偏分散$ W $の分布のグラフに変えてしまおう。

\image{./figures4/figures4b.png,不偏分散の分布のグラフ}

　$ n=1 $の曲線は、0 での割り算になるので存在しない。
　これは分布のグラフなので全範囲で積分したときに 1 になるようにグラフの高さをそれぞれ変えるべきだが、
線が重なって分かりにくくなるのでそれはやっていない。
　雰囲気を見てもらいたいだけなのだ。
　測定回数$ n $が増えるほどピークの横軸が 1 に近付く。
　これは元々標準偏差が 1、つまり分散も 1 であるような分布から抜き出してくるという設定なので当然だ。
　元々ピークが$ n-2 $になるところを$ n-1 $で割って縮めているので、
　$ (n-2)/(n-1) $、すなわち$ 1 - \frac{1}{n-1} $となり、確かにそうなることが分かるだろう。

<P>

　さて、我々は不偏分散$ W $をさらに$ n $で割って$ X $を算出したのだった。
　次のグラフは横軸のスケールを$ 1/n $に縮めたものである。
　つまり、それぞれの曲線ごとに縮め方を変えるのであり、$ n $が大きいほどギュッと縮めてある。
　左へ寄り過ぎるグラフになるので、原点近くの細かい挙動を観察しやすいように全体的に拡大してある。

\image{./figures4/figures4c.png,不偏分散の分布のグラフのスケールを変えてみた図}

　これは偶然に頼って$ X $の値を一度だけ得るときの分布を表していると言えるだろう。
　確率分布を表すのなら各曲線が作る面積が 1 になるようにしておくべきだが、相変わらず各曲線の係数の調整はしていない。

\note{　本当に見たければ各自でやってみてくれ。}

　$ n $が大きいほどピークが左へ寄る傾向が見えてきている。
　当たり前といえば当たり前だが、この性質を見たかったので一安心だ。
　しかしグラフのピークから右側への分布がなだらかに長く続くところが気になる。
　これでは大き過ぎる値を引いてしまう可能性が高いことになるからだ。
　$ n=5 $くらいになるとそれもあまり気にならないくらいにはなってくるけれども、一度気になってしまったものは気になって仕方がない。

<P>

　しかし、我々が実際に誤差$ \sigma $として併記するために使うのは$ \sqrt{X} $なのであった。
　この平方根を考慮してグラフを書き直してやったら右側への坂道も引き締まるのではなかろうか。
　横軸が平均値の標準偏差$ \sigma $になるように変更してやろう。

\image{./figures4/figures4d.png,横軸が標準偏差になるように不偏分散の分布のグラフのスケールを変えてみた図}

　これは元々の横軸の 4 の辺りの確率密度を 2 の辺りに、9 の辺りの値を 3 に、16 の辺りを 4 に来るように書き直しただけである。
　つまり、ぐっと左に寄ってくることになる。
　左に詰まって見にくくなったので、またスケールを変更して原点付近を拡大してある。

<P>

　この結果はなかなか良いのではないだろうか？
　左右対称に近い感じに引き締まってきた。
　今までは$ n=6 $までの曲線しか描いてこなかったが、ここではさらに増やすとどうなるかを書き加えてある。
　$ n $が増えるほどグラフのピークが左へ寄っていっているのは標準偏差が実際に小さくなって行っているのであり、
これは測定回数を増やすほど真の値を正確に推定することができるようになっていっていることを意味する。
　また、$ n $が増えるほど曲線の横幅がだんだんと狭くなっていっていることも分かるだろう。
　もしちゃんと全体の積分が 1 になるように調整すれば$ n $が大きいほど上へ鋭く突き抜けた形に描かれるはずで、
ここではやらないけれども、もしそれを見ればイメージも変わるはずだ。

<P>

　我々が本当に欲しいのは、偶然に頼って得る$ X $ではなく、それを無限に繰り返して得られる$ E[X] $であり、
もしできるなら$ \sigma = \sqrt{E[X]} $として測定値の平均の値に併記したいのである。
　現実的には無理だから$ \sigma = \sqrt{X} $を仕方なく使っているのである。
　このグラフでは曲線が左右対称ではないから多少は違うけれども、ピーク辺りが$ \sqrt{E[X]} $を意味していると思われる。
　そこで気になるのは、確かに曲線の横幅は$ n $が増えるほど減っていくけれども、それは$ \sqrt{E[X]} $に比べてどうなのかということである。
　得たいと思っている値に比べてのばらつき具合はどう変化しているだろう？
　そこで、ピークの位置が揃うように曲線ごとの横幅の比率を変えてみよう。

\image{./figures4/figures4e.png,不偏分散の分布のグラフのスケールをさらに変えてみた図}

　$ n=2 $の曲線にはピークがないので消した。
　$ n $が大きいほど、実際よりも横に大きく引き伸ばしてあることになるのだが、それでも横幅が徐々に狭くなっていっているのが分かる。
　ピークの位置がおおよそ$ \sqrt{E[X]} $だろうと考えられ、そこが本当にほしい$ \sigma $の値だが、
そこから何倍も外れた値が出てしまう可能性はとても小さくなっていっている。
　$ n=5 $では多少心配だが、$ n=10 $くらいならもっと安心できる。

<P>

　記事を書く前はもっと鋭い形になることを期待していたのだが実際にやってみるとそうでもなかった。
　測定回数を増やしても急に幅が狭まるわけでもないし、これくらいの幅があることを覚悟して使うしかないのだろう。
　桁は合っている、と言えるくらいのものだ。
　平均値の標準偏差は測定回数に依存するようなものであり、物理的な対象に関する値ではない。
　測定の質を表すデータである。
　大体の目安を意味する数字であったのだからこれくらいでも仕方ないのだろう。

